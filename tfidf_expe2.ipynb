{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tfidf_expe2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPyb/1AOOB3JKXKimEHvGmZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"71uEq7gA7G1U"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import ast\n","import os\n","import pickle\n","import shutil\n","import random"]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"a1BXwbsj7KNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read the whole database after filtering documents\n","df_base = pd.read_csv('../resources/database_doc_type.csv')\n","df_base.head()"],"metadata":{"id":"716Rx2EE7LRP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# filter doc_type (only keep CR:CRH-HOSPI)\n","# base 3\n","df_base = df_base[df_base['doc_type']=='CR:CRH-HOSPI']\n","len(df_base)"],"metadata":{"id":"Z7O_JEmz7Mjr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_base_new = df_base.drop_duplicates(subset=['observation_blob'])\n","df_base_new.dropna(subset=['observation_blob'],inplace=True)\n","df_base_new.reset_index(inplace=True)\n","df_base_new.drop(columns=['index','Unnamed: 0'],inplace=True)\n","df_base_new.reset_index(inplace=True)\n","df_base_new.rename(columns={'index':'source'},inplace=True)\n","df_base_new"],"metadata":{"id":"Jxub2ozs7N8p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# database too large, choose 10000 docs randomly to form a smaller database\n","df_base_small = df_base_new.sample(n=10000,replace=False, random_state=22)"],"metadata":{"id":"lQSyO0RG7PHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# randomly select 100 samples\n","df_sample = df_base_small.sample(n=100, replace=False, random_state=2022)\n","df_sample"],"metadata":{"id":"qOt7Hh577Qzl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the rest docs form a databse\n","df_database = pd.concat([df_base_small,df_sample,df_sample]).drop_duplicates(keep=False)\n","df_database"],"metadata":{"id":"ZVzz6jeD7SI2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save database and samples\n","df_database.to_pickle(\"./df_database.pkl\")\n","df_sample.to_pickle(\"./df_sample.pkl\")"],"metadata":{"id":"lo3vHeDH7Tqs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_sources = list(df_sample['source'])\n","base_sources = list(df_database['source'])"],"metadata":{"id":"4tW__lsi7U_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nb of letters to consider in cim10 codes\n","nb_letter = 3\n","# dic where keys are sources and values are cim10 codes for this doc\n","dic_source_cim10 = {}\n","for i in range(len(df_base_new)):\n","    source = df_base_new.loc[i,'source']\n","    cim10s = ast.literal_eval(df_base_new.loc[i,'list_cim10'])\n","    # cim10s = list(set([x.split(':')[-1][:nb_letter] for x in cim10s])) # at least one same DP/DAS\n","    cim10s = list(set([x.split(':')[-1][:nb_letter] for x in cim10s if x.startswith('DP')])) # at least one same DP\n","    # cim10s = list(set([x.split(':')[-1][:nb_letter] for x in cim10s if x.startswith('DAS')])) # at least one same DAS\n","    dic_source_cim10[source] = cim10s"],"metadata":{"id":"qJ0bMDff7WGH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TF-IDF\n","use TF-IDF to find top k candidates for each sample in database, see how many of these candidates have at least one same DP/DAS as the sample itself."],"metadata":{"id":"jJaJxR1T7XXZ"}},{"cell_type":"code","source":["vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(list(df_sample['observation_blob'])+list(df_database['observation_blob']))\n","vectors = X.toarray()\n","vector_sample = vectors[:len(list(df_sample['observation_blob']))]\n","vector_base = vectors[len(list(df_sample['observation_blob'])):]"],"metadata":{"id":"FNG4_vWp7YsQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for each sample, find top nb candidates from database\n","nb = 100\n","\n","sim = np.matmul(vector_sample,vector_base.T)\n","candidates = torch.topk(torch.tensor(sim), k=nb, dim=1, sorted=True).indices\n","pred_cands = [[base_sources[idx] for idx in candidate] for candidate in candidates]"],"metadata":{"id":"K_L-eXBO7Z5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dic_sample_cands = {}\n","for i in range(len(sample_sources)):\n","    dic_sample_cands[sample_sources[i]] = pred_cands[i]"],"metadata":{"id":"v_b42Wz67a2P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"nt0EVSOY7cKw"}},{"cell_type":"code","source":["accs = []\n","for sample in tqdm(dic_sample_cands):\n","    sample_cim10 = dic_source_cim10[sample]\n","    hit = 0\n","    for cand in dic_sample_cands[sample]:\n","        cand_cim10 = dic_source_cim10[cand]\n","        if len([x for x in cand_cim10 if x in sample_cim10])>0:\n","            hit+=1\n","    acc = hit/len(dic_sample_cands[sample])\n","    accs.append(acc)"],"metadata":{"id":"Q088gaAi7dZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.mean(accs)"],"metadata":{"id":"IVn3HK8j7fs8"},"execution_count":null,"outputs":[]}]}